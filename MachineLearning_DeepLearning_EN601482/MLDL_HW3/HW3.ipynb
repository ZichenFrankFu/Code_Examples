{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMeU0+74Jreygox7QpJWUbk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Question 2 (b)**"],"metadata":{"id":"DjMXVY1NlzMC"}},{"cell_type":"markdown","metadata":{"id":"scoOJkUyNrin"},"source":["### Imports\n","\n","You should be able to complete the entire assignment using only the following imports. Please consult the course staff if you are unsure about whether additional packages may be used."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"9vllQSx4E9oP","executionInfo":{"status":"ok","timestamp":1695655206306,"user_tz":240,"elapsed":531,"user":{"displayName":"Frank Fu","userId":"00524219313507346699"}}},"outputs":[],"source":["## Import Packages\n","import random\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"7saLrsq-5Wc6","executionInfo":{"status":"ok","timestamp":1695655662659,"user_tz":240,"elapsed":523,"user":{"displayName":"Frank Fu","userId":"00524219313507346699"}}},"outputs":[],"source":["#from os import O_SEQUENTIAL\n","class Value:\n","\n","    \"\"\"\n","    Basic unit of storing a single scalar value and its gradient\n","    \"\"\"\n","\n","    def __init__(self, data, _children=()):\n","        \"\"\"\n","\n","        \"\"\"\n","        self.data = data\n","        self.grad = 0\n","        self._prev = set(_children)\n","        self._backward = lambda: None\n","\n","    def __add__(self, other):\n","        \"\"\"\n","        Example implementation of a single class operation (addition)\n","\n","        Args:\n","            other (Any): Node to add with the class\n","\n","        Returns:\n","            out (callable): Function to referesh the gradient\n","        \"\"\"\n","        #Firstly, convert some default value type in python to Value\n","        #Then do operations with two or more Value object\n","        other = other if isinstance(other, Value) else Value(other)\n","\n","        #Secondly, create a new Value object which is the result of the operation\n","        out = Value(self.data + other.data, (self, other))\n","\n","        #Thirdly, create a _backward function for the output object to refresh\n","        # the gradient of its _childrens,\n","        #Then assign this _backward function to the output object.\n","        def _backward():\n","            self.grad += out.grad * 1.0\n","            other.grad += out.grad * 1.0\n","        out._backward = _backward\n","\n","        return out\n","\n","    def __mul__(self, other):\n","        \"\"\"\n","        Multiplication operation (e.g. Value(3) * Value(2) = Value(6))\n","        \"\"\"\n","        #Convert default value type to Value\n","        other = other if isinstance(other, Value) else Value(other)\n","\n","        #Create a new Value object with multiplied value\n","        out = Value(self.data * other.data, (self, other))\n","\n","        #_backward function will pass the coefficient times the gradient from children nodes\n","        def _backward():\n","            self.grad += out.grad * other.data\n","            other.grad += out.grad * self.data\n","        out._backward = _backward\n","\n","        return out\n","\n","    def __pow__(self, other):\n","        \"\"\"\n","        Power operation (e.g Value(3) ** 2 = Value(9))\n","        \"\"\"\n","        #TODO implement power operation, we don't need to convert the exponent to Value\n","        assert isinstance(other, (int, float))\n","\n","        #Create a new Value object with multiplied value\n","        out = Value(pow(self.data, other), {self})\n","\n","        #_backward function will pass polynomial derivative from children node\n","        def _backward():\n","          if other == 1:\n","            self.grad = out.grad\n","          elif other == 0:\n","            self.grad = 0\n","          else:\n","            self.grad += out.grad * (other * pow(self.data, other-1))\n","        out._backward = _backward\n","\n","        return out\n","\n","    def relu(self):\n","        \"\"\"\n","        ReLU activation function applied to the current Value\n","        \"\"\"\n","        #TODO implement the relu activation function for the value itself.\n","        out = Value(max(self.data, 0), (self,))\n","\n","        def _backward():\n","            self.grad += out.grad if self.data > 0 else 0\n","        out._backward = _backward\n","\n","        return out\n","\n","\n","    def exp(self):\n","        \"\"\"\n","        Exponentiate the current Value (e.g. e ^ Value(0) = Value(1))\n","        \"\"\"\n","        #TODO implement the exponential function for and treat the value as exponent.\n","        #The base is natural e, you can use numpy to calculate the value of the exponential.\n","\n","        #Create a new Value object with exp-ed value\n","        #float128 to avoid overflow\n","        exp_val = np.exp(np.float128(self.data))\n","        out = Value(exp_val, (self,))\n","\n","        #_backward function will pass exp derivative to children node\n","        def _backward():\n","          self.grad += out.grad * exp_val\n","        out._backward = _backward\n","\n","        return out\n","\n","    def log(self):\n","        \"\"\"\n","        Take the natural logarithm (base e) of the current Value\n","        \"\"\"\n","        #TODO implement the logarithm function for and treat the value as exponent.\n","        #The bottom number should be e, you can use numpy to calculate the value of the logarithm.\n","\n","        #Create a new Value object with log-ed value\n","        out = Value(np.log(self.data), {self})\n","\n","        #_backward function will pass log derivative to children node\n","        def _backward():\n","          self.grad += out.grad * (1/self.data)\n","        out._backward = _backward\n","\n","        return out\n","\n","    def sigmoid(self):\n","        out = Value(1/(1+(np.exp(-self.data))), {self})\n","\n","        #_backward function will pass log derivative to children node\n","        def _backward():\n","          self.grad += out.grad * (out.data) * (1-out.data)\n","        out._backward = _backward\n","\n","        return out\n","\n","\n","    def topoSort(self, topo, visited):\n","      for nodes in self._prev:\n","        if not nodes in visited:\n","          nodes.topoSort(topo, visited)\n","      topo.insert(0, self)\n","      visited.add(self)\n","\n","\n","\n","    def backward(self):\n","        \"\"\"\n","        Run backpropagation from the current Value\n","        \"\"\"\n","        #This function is called when you start backpropagation from this Value\n","\n","        #The gradient of this value is initialized to 1 for you.\n","        self.grad = 1\n","\n","        #You need to find a right topological order all of the children in the graph.\n","        #As for topology sort, you can refer to http://www.cs.cornell.edu/courses/cs312/2004fa/lectures/lecture15.htm\n","        \"\"\" using stack is computationally expansive, making the runtime increased severely\n","        def topoSort(value, visited, stack):\n","          visited.append(value)\n","          for node in value._prev:\n","            if node not in visited:\n","              topoSort(node, visited, stack)\n","          stack.append(0, value)\n","\n","        def topoList(value, topo):\n","          stack = []\n","          visited = []\n","          topoSort(value, visited, stack)\n","          while(len(stack) != 0):\n","            v = stack.pop()\n","            topo.append(v)\n","        \"\"\"\n","\n","        topo = []\n","        visited = []\n","        #TODO find the right list of Value to be traversed\n","        '''\n","        Hint: you can recursively visit all non-visited node from the node calling backward.\n","        add one node to the head of the list after all of its children node are visited\n","        '''\n","        self.topoSort(topo, set())\n","\n","        #topoSort(self, visited, topo)\n","\n","        #go one variable at a time and apply the chain rule to get its gradient\n","        for v in topo:\n","            v._backward()\n","\n","    # We handled the negation and reverse operations for you\n","    def __neg__(self): # -self\n","        \"\"\"\n","        Negate the current Value\n","        \"\"\"\n","        return self * -1\n","\n","    def __radd__(self, other): #other + self\n","        \"\"\"\n","        Reverse addition operation (ordering matters in Python)\n","        \"\"\"\n","        return self + other\n","\n","    def __sub__(self, other): # self - other\n","        \"\"\"\n","        Subtraction operation\n","        \"\"\"\n","        return self + (-other)\n","\n","    def __rsub__(self, other): # other - self\n","        \"\"\"\n","        Reverse subtraction operation\n","        \"\"\"\n","        return other + (-self)\n","\n","    def __rmul__(self, other): # other * self\n","        \"\"\"\n","        Reverse multiplication operation\n","        \"\"\"\n","        return self * other\n","\n","    def __truediv__(self, other): # self / other\n","        \"\"\"\n","        Division operation\n","        \"\"\"\n","        return self * other**-1\n","\n","    def __rtruediv__(self, other): # other / self\n","        \"\"\"\n","        Reverse diction operation\n","        \"\"\"\n","        return other * self**-1\n","\n","    def __repr__(self):\n","        \"\"\"\n","        Class representation (instead of unfriendly memory address)\n","        \"\"\"\n","        return f\"Value(data={self.data}, grad={self.grad})\"\n","\n","    # extra function to make Value object comparable for finding the max\n","    def __gt__(self, other):\n","        return self.data > other.data"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":381,"status":"ok","timestamp":1695655665818,"user":{"displayName":"Frank Fu","userId":"00524219313507346699"},"user_tz":240},"id":"PeoL0WIL_sHD","outputId":"da93d497-f7c5-48a2-ef94-732db506a499"},"outputs":[{"output_type":"stream","name":"stdout","text":["multiply1 Value(data=0.1575, grad=0.00216451138525994)\n","multiply2 Value(data=-0.059322318648819546, grad=-0.07919863670687821)\n","multiply3 Value(data=0.37843553907524874, grad=-0.4065042813639045)\n","sig1 Value(data=0.5392938058983595, grad=0.008711850037756604)\n","sig2 Value(data=0.4851737680451907, grad=-0.3170733394638455)\n","sig3 Value(data=0.5934957186360955, grad=-1.6849321209899988)\n","loss Value(data=0.5217252787139592, grad=1)\n","w1 Value(data=0.25, grad=0.001363642172713762)\n","w2 Value(data=-0.11, grad=-0.04271133421161387)\n","w3 Value(data=0.78, grad=-0.19722521391582792)\n","x Value(data=0.63, grad=0.000541127846314985)\n"]}],"source":["nodes = []\n","names = []\n","## Initialize Example Values (From Written Assignment)\n","x = Value(0.63)\n","w1 = Value(0.25)\n","w2 = Value(-0.11)\n","w3 = Value(0.78)\n","\n","# Initialize the computational graph\n","multiply1 = w1.__mul__(x)\n","sig1 = multiply1.sigmoid()\n","multiply2 = w2.__mul__(sig1)\n","sig2 = multiply2.sigmoid()\n","multiply3 = w3.__mul__(sig2)\n","sig3 = multiply3.sigmoid()\n","loss = -sig3.log()\n","nodes.append(multiply1)\n","nodes.append(multiply2)\n","nodes.append(multiply3)\n","nodes.append(sig1)\n","nodes.append(sig2)\n","nodes.append(sig3)\n","nodes.append(loss)\n","names.append(\"multiply1\")\n","names.append(\"multiply2\")\n","names.append(\"multiply3\")\n","names.append(\"sig1\")\n","names.append(\"sig2\")\n","names.append(\"sig3\")\n","names.append(\"loss\")\n","\n","# Calculate back propagation\n","loss.backward()\n","\n","\n","#TODO\n","#Do calculation for the question 1.b, and call backward to start backpropagation.\n","#Then print out the gradient of w1 w2 x1 x2.\n","for i in range(len(nodes)):\n","  print(names[i], nodes[i])\n","\n","print(\"w1\",w1)\n","print(\"w2\",w2)\n","print(\"w3\",w3)\n","print(\"x\",x)\n"]},{"cell_type":"code","source":["from numpy.lib.function_base import add_newdoc_ufunc\n","nodes = []\n","names = []\n","## Initialize Example Values (From Written Assignment)\n","x = Value(0.63)\n","w1 = Value(0.25)\n","w2 = Value(-0.11)\n","w3 = Value(0.78)\n","\n","# Initialize the computational graph\n","multiply1 = w1.__mul__(x)\n","sig1 = multiply1.sigmoid()\n","multiply2 = w2.__mul__(sig1)\n","sig2 = multiply2.sigmoid()\n","multiply3 = w3.__mul__(sig2)\n","add = multiply3.__add__(-128)\n","loss = add.__pow__(2)\n","nodes.append(multiply1)\n","nodes.append(multiply2)\n","nodes.append(multiply3)\n","nodes.append(sig1)\n","nodes.append(sig2)\n","\n","nodes.append(loss)\n","names.append(\"multiply1\")\n","names.append(\"multiply2\")\n","names.append(\"multiply3\")\n","names.append(\"sig1\")\n","names.append(\"sig2\")\n","\n","names.append(\"loss\")\n","\n","# Calculate back propagation\n","loss.backward()\n","\n","\n","#TODO\n","#Do calculation for the question 1.b, and call backward to start backpropagation.\n","#Then print out the gradient of w1 w2 x1 x2.\n","for i in range(len(nodes)):\n","  print(names[i], nodes[i])\n","\n","print(\"w1\",w1)\n","print(\"w2\",w2)\n","print(\"w3\",w3)\n","print(\"x\",x)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lPSHb7rl2RrE","executionInfo":{"status":"ok","timestamp":1695655758727,"user_tz":240,"elapsed":527,"user":{"displayName":"Frank Fu","userId":"00524219313507346699"}},"outputId":"5bcf9218-5fa1-4e64-9d19-9a4e7838d1f0"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["multiply1 Value(data=0.1575, grad=1.35909185681155)\n","multiply2 Value(data=-0.059322318648819546, grad=-49.7286468215865)\n","multiply3 Value(data=0.37843553907524874, grad=-255.2431289218495)\n","sig1 Value(data=0.5392938058983595, grad=5.470151150374515)\n","sig2 Value(data=0.4851737680451907, grad=-199.0896405590426)\n","loss Value(data=16287.26371545397, grad=1)\n","w1 Value(data=0.25, grad=0.8562278697912765)\n","w2 Value(data=-0.11, grad=-26.818351206588744)\n","w3 Value(data=0.78, grad=-123.83727062665811)\n","x Value(data=0.63, grad=0.3397729642028875)\n"]}]}]}