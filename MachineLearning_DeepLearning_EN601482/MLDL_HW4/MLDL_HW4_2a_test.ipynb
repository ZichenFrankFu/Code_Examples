{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOdzgzzkndGmEFo5Pylmm68"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["## External Libararies\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from matplotlib.patches import Polygon\n","from matplotlib.collections import PatchCollection"],"metadata":{"id":"4DTQG1NlpwP5","executionInfo":{"status":"ok","timestamp":1696912955416,"user_tz":240,"elapsed":244,"user":{"displayName":"Frank Fu","userId":"00524219313507346699"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["## Additional External Libraries (Deep Learning)\n","import torch\n","import torch.nn as nn\n","import random\n","from torch.autograd import Variable\n","from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n","from torchvision import transforms as tfs\n","from PIL import Image\n","from torchvision.datasets import FashionMNIST"],"metadata":{"id":"uEkum-wKSLoS","executionInfo":{"status":"ok","timestamp":1696912955729,"user_tz":240,"elapsed":2,"user":{"displayName":"Frank Fu","userId":"00524219313507346699"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["# Hyperparameter (Feel free to make modifications)\n","TRAIN_BATCH_SIZE = 50\n","VAL_BATCH_SIZE = 50\n","TEST_BATCH_SIZE = 1\n","\n","# Transform data to PIL images\n","transforms = tfs.Compose([tfs.ToTensor()])\n","\n","# Train/Val Subsets\n","train_mask = range(50000)\n","val_mask = range(50000, 60000)\n","\n","# Download/Load Dataset\n","train_dataset = FashionMNIST('./data', train=True, transform=transforms, download=True)\n","test_dataset = FashionMNIST('./data', train=False, transform=transforms, download=True)\n","\n","# Data Loaders\n","train_dataloader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, sampler=SubsetRandomSampler(train_mask))\n","val_dataloader = DataLoader(train_dataset, batch_size=VAL_BATCH_SIZE, sampler=SubsetRandomSampler(val_mask))\n","test_dataloader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE)"],"metadata":{"id":"3DXeWUTDSMXn","executionInfo":{"status":"ok","timestamp":1696912956001,"user_tz":240,"elapsed":273,"user":{"displayName":"Frank Fu","userId":"00524219313507346699"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["class CNNet_2a(nn.Module):\n","\n","    def __init__(self, seed_value):\n","        \"\"\"\n","\n","        \"\"\"\n","        ## Inherent Torch Module\n","        super(CNNet_2a, self).__init__()\n","\n","        # First conv layer with ReLU and MaxPool: 28*28*1 -> 28*28*32 -> 14*14*32\n","        self.conv1 = nn.Sequential(\n","            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2)\n","        )\n","\n","        # Second conv layer with ReLU and MaxPool: 14*14*32 -> 12*12*64 -> 6*6*64\n","        self.conv2 = nn.Sequential(\n","            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2)\n","        )\n","\n","\n","        # Third fc layer with sigmoid activation\n","        self.fc1 = nn.Linear(6*6*64, 600)\n","        self.sig1 = nn.Sigmoid()\n","\n","        # Fourth fc layer with sigmoid activation\n","        self.fc2 = nn.Linear(600, 120)\n","        self.sig2 = nn.Sigmoid()\n","\n","        # Output layer\n","        self.output = nn.Linear(120, 10)\n","\n","        self._initialize_weights(seed_value)\n","\n","\n","    def _initialize_weights(self, seed_value):\n","        \"\"\"\n","        Initialize the weights and biases of the model.\n","        \"\"\"\n","        torch.manual_seed(seed_value)\n","        torch.nn.init.xavier_uniform_(self.conv1[0].weight)\n","        torch.nn.init.xavier_uniform_(self.conv2[0].weight)\n","\n","        torch.manual_seed(seed_value)\n","        torch.nn.init.xavier_uniform_(self.fc1.weight)\n","        torch.nn.init.normal_(self.fc1.bias)\n","        torch.nn.init.xavier_uniform_(self.fc2.weight)\n","        torch.nn.init.normal_(self.fc2.bias)\n","        torch.nn.init.xavier_uniform_(self.output.weight)\n","        torch.nn.init.normal_(self.output.bias)\n","\n","\n","    def forward(self, x):\n","        \"\"\"\n","        \"\"\"\n","        ##TODO: Setup Forward Pass\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.sig1(self.fc1(x))\n","        x = self.sig2(self.fc2(x))\n","        x = self.output(x)\n","\n","        return x"],"metadata":{"id":"opZKDAgKRs3v","executionInfo":{"status":"ok","timestamp":1696912956001,"user_tz":240,"elapsed":2,"user":{"displayName":"Frank Fu","userId":"00524219313507346699"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","execution_count":25,"metadata":{"id":"Uvm_wH4DIqV8","executionInfo":{"status":"ok","timestamp":1696912956001,"user_tz":240,"elapsed":2,"user":{"displayName":"Frank Fu","userId":"00524219313507346699"}}},"outputs":[],"source":["def train(model,\n","          loss_f,\n","          optimizer,\n","          n_epoch=50,\n","          train_dataloader=train_dataloader,\n","          val_dataloader=val_dataloader,\n","          test_dataloader=test_dataloader,\n","          seed_value=None):\n","  # Define lists to store training and validation losses\n","  train_losses = []\n","  val_losses = []\n","  train_accs = []\n","  val_accs = []\n","\n","  best_model = model\n","  best_train_acc = -1\n","  best_val_acc = -1\n","  best_ind = -1\n","\n","  # Training loop\n","  for epoch in range(n_epoch):\n","      model.train()\n","      train_loss = 0.0\n","      train_correct = 0\n","\n","      # Iterate through the training data\n","      for inputs, labels in train_dataloader:\n","          # Forward pass\n","          output = model(inputs)\n","\n","          # Compute loss\n","          loss = loss_f(output, labels)\n","\n","          # Backpropagation\n","          optimizer.zero_grad()  # Zero the gradients\n","          loss.backward()\n","          optimizer.step()\n","\n","          # Update running training loss\n","          train_loss += loss.item()\n","          predict = output.argmax(axis=1)\n","          train_correct += (predict == labels).float().sum()\n","\n","      # Compute average training loss for the epoch\n","      avg_train_loss = train_loss / 50000\n","      train_losses.append(avg_train_loss)\n","      train_acc = train_correct / 50000\n","      train_accs.append(train_acc)\n","\n","      # Validation\n","      model.eval()\n","      val_loss = 0.0\n","      val_correct = 0\n","      with torch.no_grad():\n","          for inputs, labels in val_dataloader:\n","              output = model(inputs)\n","              loss = loss_f(output, labels)\n","              val_loss += loss.item()\n","              predict = output.argmax(axis=1)\n","              val_correct += (predict == labels).float().sum()\n","\n","      avg_val_loss = val_loss / 10000\n","      val_losses.append(avg_val_loss)\n","      val_acc = val_correct / 10000\n","      val_accs.append(val_acc)\n","\n","      # Record the best model\n","      if val_acc > best_val_acc:\n","        best_model = model\n","        best_ind = epoch\n","        best_train_acc = train_acc\n","        best_val_acc = val_acc\n","\n","      print(f'Epoch [{epoch+1}/{n_epoch}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n","      print(f'Train accuracy: {train_acc:.4f}, Val accuracy: {val_acc:.4f}')\n","\n","  # Test\n","  model.eval()\n","  test_loss = 0.0\n","  test_correct = 0\n","  with torch.no_grad():\n","      for inputs, labels in test_dataloader:\n","          output = best_model(inputs)\n","          loss = loss_f(output, labels)\n","          test_loss += loss.item()\n","          predict = output.argmax(axis=1)\n","          test_correct += (predict == labels).float().sum()\n","\n","  best_test_loss = test_loss / len(test_dataloader)\n","  best_test_acc = test_correct / len(test_dataloader)\n","\n","  print(f'Best Model achieved at epoch: {best_ind:.4f}')\n","  print(f'train accuracy: {best_train_acc:.4f}, validation accuracy: {best_val_acc:.4f}, test accuracy: {best_test_acc:.4f}')\n","\n","\n","  return train_losses, val_losses, train_accs, val_accs, best_model, best_ind, best_train_acc, best_val_acc, best_test_acc"]},{"cell_type":"code","source":["n_epoch = 50\n","lr = 0.5\n","seed_Conv = random.randint(0, 10000)\n","model = CNNet_2a(seed_Conv)\n","optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n","loss_f = nn.CrossEntropyLoss()"],"metadata":{"id":"csUtGRzMSeq1","executionInfo":{"status":"ok","timestamp":1696912956001,"user_tz":240,"elapsed":1,"user":{"displayName":"Frank Fu","userId":"00524219313507346699"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["## Run Training Loop\n","# Result\n","train_loss_epochs = []\n","valid_accs_epochs = []\n","\n","train_losses, val_losses, train_accs, val_accs, best_model, best_ind, best_train_acc, best_val_acc, best_test_acc = train(\n","    model, loss_f, optimizer, n_epoch = n_epoch, seed_value = seed_Conv\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L0XWiXdpShd2","outputId":"afd95534-158e-446a-b71d-cd6784ddbd81"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/50], Train Loss: 0.0154, Val Loss: 0.0108\n","Train accuracy: 0.7022, Val accuracy: 0.7881\n","Epoch [2/50], Train Loss: 0.0083, Val Loss: 0.0073\n","Train accuracy: 0.8441, Val accuracy: 0.8649\n","Epoch [3/50], Train Loss: 0.0069, Val Loss: 0.0070\n","Train accuracy: 0.8700, Val accuracy: 0.8685\n","Epoch [4/50], Train Loss: 0.0062, Val Loss: 0.0062\n","Train accuracy: 0.8822, Val accuracy: 0.8836\n","Epoch [5/50], Train Loss: 0.0057, Val Loss: 0.0057\n","Train accuracy: 0.8927, Val accuracy: 0.8943\n","Epoch [6/50], Train Loss: 0.0053, Val Loss: 0.0057\n","Train accuracy: 0.8995, Val accuracy: 0.8916\n","Epoch [7/50], Train Loss: 0.0049, Val Loss: 0.0057\n","Train accuracy: 0.9072, Val accuracy: 0.8922\n","Epoch [8/50], Train Loss: 0.0046, Val Loss: 0.0052\n","Train accuracy: 0.9128, Val accuracy: 0.9058\n","Epoch [9/50], Train Loss: 0.0042, Val Loss: 0.0053\n","Train accuracy: 0.9192, Val accuracy: 0.9063\n","Epoch [10/50], Train Loss: 0.0039, Val Loss: 0.0052\n","Train accuracy: 0.9249, Val accuracy: 0.9074\n"]}]},{"cell_type":"code","source":["epochs = [*range(0, 50, 1)]\n","loss_list = [train_losses, val_losses]\n","acc_list = [train_accs, val_accs]\n","\n","fig, axs = plt.subplots(1, 2,figsize=(20, 7))\n","for losses in loss_list:\n","  axs[0].plot(epochs, losses)\n","axs[0].set_title('Training Loss')\n","axs[0].legend(['train', 'validation'])\n","for accs in acc_list:\n","  axs[1].plot(epochs, accs)\n","axs[1].set_title('Testing Accuracy')\n","axs[1].legend(['train', 'validation'])\n","\n","\n","plt.show()"],"metadata":{"id":"wnJjAgrzpzHu"},"execution_count":null,"outputs":[]}]}