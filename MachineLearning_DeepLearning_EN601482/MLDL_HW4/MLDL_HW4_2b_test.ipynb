{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNT+0yGgppjlQlOOrcTy/hE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["## External Libararies\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from matplotlib.patches import Polygon\n","from matplotlib.collections import PatchCollection"],"metadata":{"id":"UEHqDTEVkywp","executionInfo":{"status":"ok","timestamp":1696910421527,"user_tz":240,"elapsed":122,"user":{"displayName":"Frank Fu","userId":"00524219313507346699"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["## Additional External Libraries (Deep Learning)\n","import torch\n","import torch.nn as nn\n","import random\n","from torch.autograd import Variable\n","from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n","from torchvision import transforms as tfs\n","from PIL import Image\n","from torchvision.datasets import FashionMNIST"],"metadata":{"id":"57GUcbv1jeaK","executionInfo":{"status":"ok","timestamp":1696910421645,"user_tz":240,"elapsed":2,"user":{"displayName":"Frank Fu","userId":"00524219313507346699"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","execution_count":36,"metadata":{"id":"iENRSyFyjUsr","executionInfo":{"status":"ok","timestamp":1696910421789,"user_tz":240,"elapsed":146,"user":{"displayName":"Frank Fu","userId":"00524219313507346699"}}},"outputs":[],"source":["# Hyperparameter (Feel Free to Change These, but Make Sure your Training Loop Still Works as Expected)\n","TRAIN_BATCH_SIZE = 50\n","VAL_BATCH_SIZE = 50\n","TEST_BATCH_SIZE = 1\n","\n","# Transform data to PIL images\n","transforms = tfs.Compose([tfs.ToTensor()]) ##TODO: Use the same from above or consider alternatives\n","\n","# Train/Val Subsets\n","train_mask = range(50000)\n","val_mask = range(50000, 60000)\n","\n","# Download/Load Dataset\n","train_dataset = FashionMNIST('./data', train=True, transform=transforms, download=True)\n","test_dataset = FashionMNIST('./data', train=False, transform=transforms, download=True)\n","\n","# Data Loaders\n","train_dataloader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, sampler=SubsetRandomSampler(train_mask))\n","val_dataloader = DataLoader(train_dataset, batch_size=VAL_BATCH_SIZE, sampler=SubsetRandomSampler(val_mask))\n","test_dataloader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE)"]},{"cell_type":"code","source":["class CNNet_2b(nn.Module):\n","\n","    def __init__(self, seed_value):\n","        \"\"\"\n","\n","        \"\"\"\n","        ## Inherent Torch Module\n","        super(CNNet_2b, self).__init__()\n","\n","        # First conv layer with ReLU and MaxPool: 28*28*1 -> 28*28*32 -> 14*14*32\n","        self.conv1 = nn.Sequential(\n","            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2)\n","        )\n","\n","        # Second conv layer with ReLU and MaxPool: 14*14*32 -> 12*12*64 -> 6*6*64\n","        self.conv2 = nn.Sequential(\n","            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2)\n","        )\n","\n","\n","        # Third fc layer with sigmoid activation\n","        self.fc1 = nn.Linear(6*6*64, 648)\n","        self.sig1 = nn.Sigmoid()\n","        self.drop1 = nn.Dropout2d(0.25)\n","\n","        # Fourth fc layer with sigmoid activation\n","        self.fc2 = nn.Linear(648, 128)\n","        self.sig2 = nn.Sigmoid()\n","        self.drop2 = nn.Dropout2d(0.25)\n","\n","        # Output layer\n","        self.output = nn.Linear(128, 10)\n","\n","        self._initialize_weights(seed_value)\n","\n","\n","    def _initialize_weights(self, seed_value):\n","        \"\"\"\n","        Initialize the weights and biases of the model.\n","        \"\"\"\n","        torch.manual_seed(seed_value)\n","        torch.nn.init.xavier_uniform_(self.conv1[0].weight)\n","        torch.nn.init.xavier_uniform_(self.conv2[0].weight)\n","\n","        torch.manual_seed(seed_value)\n","        torch.nn.init.xavier_uniform_(self.fc1.weight)\n","        torch.nn.init.normal_(self.fc1.bias)\n","        torch.nn.init.xavier_uniform_(self.fc2.weight)\n","        torch.nn.init.normal_(self.fc2.bias)\n","        torch.nn.init.xavier_uniform_(self.output.weight)\n","        torch.nn.init.normal_(self.output.bias)\n","\n","\n","    def forward(self, x):\n","        \"\"\"\n","        \"\"\"\n","        ##TODO: Setup Forward Pass\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.drop1(self.sig1(self.fc1(x)))\n","        x = self.drop2(self.sig2(self.fc2(x)))\n","        #x = self.sig1(self.fc1(x))\n","        #x = self.sig2(self.fc2(x))\n","        x = self.output(x)\n","\n","        return x"],"metadata":{"id":"zzNCc-CJjl0J","executionInfo":{"status":"ok","timestamp":1696910421789,"user_tz":240,"elapsed":2,"user":{"displayName":"Frank Fu","userId":"00524219313507346699"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["def train(model,\n","          loss_f,\n","          optimizer,\n","          n_epoch=50,\n","          train_dataloader=train_dataloader,\n","          val_dataloader=val_dataloader,\n","          test_dataloader=test_dataloader,\n","          seed_value=None):\n","  # Define lists to store training and validation losses\n","  train_losses = []\n","  val_losses = []\n","  train_accs = []\n","  val_accs = []\n","\n","  best_model = model\n","  best_train_acc = -1\n","  best_val_acc = -1\n","  best_ind = -1\n","\n","  # Training loop\n","  for epoch in range(n_epoch):\n","      model.train()\n","      train_loss = 0.0\n","      train_correct = 0\n","\n","      # Iterate through the training data\n","      for inputs, labels in train_dataloader:\n","          # Forward pass\n","          output = model(inputs)\n","\n","          # Compute loss\n","          loss = loss_f(output, labels)\n","\n","          # Backpropagation\n","          optimizer.zero_grad()  # Zero the gradients\n","          loss.backward()\n","          optimizer.step()\n","\n","          # Update running training loss\n","          train_loss += loss.item()\n","          predict = output.argmax(axis=1)\n","          train_correct += (predict == labels).float().sum()\n","\n","      # Compute average training loss for the epoch\n","      avg_train_loss = train_loss / 50000\n","      train_losses.append(avg_train_loss)\n","      train_acc = train_correct / 50000\n","      train_accs.append(train_acc)\n","\n","      # Validation\n","      model.eval()\n","      val_loss = 0.0\n","      val_correct = 0\n","      with torch.no_grad():\n","          for inputs, labels in val_dataloader:\n","              output = model(inputs)\n","              loss = loss_f(output, labels)\n","              val_loss += loss.item()\n","              predict = output.argmax(axis=1)\n","              val_correct += (predict == labels).float().sum()\n","\n","      avg_val_loss = val_loss / 10000\n","      val_losses.append(avg_val_loss)\n","      val_acc = val_correct / 10000\n","      val_accs.append(val_acc)\n","\n","      # Record the best model\n","      if val_acc > best_val_acc:\n","        best_model = model\n","        best_ind = epoch\n","        best_train_acc = train_acc\n","        best_val_acc = val_acc\n","\n","      print(f'Epoch [{epoch+1}/{n_epoch}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n","      print(f'Train accuracy: {train_acc:.4f}, Val accuracy: {val_acc:.4f}')\n","\n","  # Test\n","  model.eval()\n","  test_loss = 0.0\n","  test_correct = 0\n","  with torch.no_grad():\n","      for inputs, labels in test_dataloader:\n","          output = best_model(inputs)\n","          loss = loss_f(output, labels)\n","          test_loss += loss.item()\n","          predict = output.argmax(axis=1)\n","          test_correct += (predict == labels).float().sum()\n","\n","  best_test_loss = test_loss / len(test_dataloader)\n","  best_test_acc = test_correct / len(test_dataloader)\n","\n","  print(f'Best Model achieved at epoch: {best_ind:.4f}')\n","  print(f'train accuracy: {best_train_acc:.4f}, validation accuracy: {best_val_acc:.4f}, test accuracy: {best_test_acc:.4f}')\n","\n","\n","  return train_losses, val_losses, train_accs, val_accs, best_model, best_ind, best_train_acc, best_val_acc, best_test_acc"],"metadata":{"id":"REy-RlBQlDMl","executionInfo":{"status":"ok","timestamp":1696910421934,"user_tz":240,"elapsed":147,"user":{"displayName":"Frank Fu","userId":"00524219313507346699"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["##TODO: Fit and evaluate your model. What do you observe?\n","n_epoch = 50\n","lr = 0.5\n","seed_Conv = random.randint(0, 10000)\n","model = CNNet_2b(seed_Conv)\n","\n","# different optimizer\n","optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n","loss_f = nn.CrossEntropyLoss()\n","\n","## Run Training Loop\n","# Result\n","\n","train_losses, val_losses, train_accs, val_accs, best_model, best_ind, best_train_acc, best_val_acc, best_test_acc = train(\n","    model, loss_f, optimizer, n_epoch, seed_value = seed_Conv\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"YrxMPU45jq0H","executionInfo":{"status":"error","timestamp":1696913651676,"user_tz":240,"elapsed":683523,"user":{"displayName":"Frank Fu","userId":"00524219313507346699"}},"outputId":"363e761b-1baf-4003-e55c-e44f58299998"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/50], Train Loss: 0.0102, Val Loss: 0.0072\n","Train accuracy: 0.8110, Val accuracy: 0.8654\n","Epoch [2/50], Train Loss: 0.0064, Val Loss: 0.0060\n","Train accuracy: 0.8828, Val accuracy: 0.8882\n","Epoch [3/50], Train Loss: 0.0054, Val Loss: 0.0057\n","Train accuracy: 0.8997, Val accuracy: 0.8929\n","Epoch [4/50], Train Loss: 0.0049, Val Loss: 0.0051\n","Train accuracy: 0.9094, Val accuracy: 0.9047\n","Epoch [5/50], Train Loss: 0.0044, Val Loss: 0.0047\n","Train accuracy: 0.9176, Val accuracy: 0.9141\n","Epoch [6/50], Train Loss: 0.0041, Val Loss: 0.0051\n","Train accuracy: 0.9243, Val accuracy: 0.9055\n","Epoch [7/50], Train Loss: 0.0038, Val Loss: 0.0045\n","Train accuracy: 0.9291, Val accuracy: 0.9154\n","Epoch [8/50], Train Loss: 0.0034, Val Loss: 0.0044\n","Train accuracy: 0.9357, Val accuracy: 0.9182\n","Epoch [9/50], Train Loss: 0.0032, Val Loss: 0.0041\n","Train accuracy: 0.9413, Val accuracy: 0.9243\n","Epoch [10/50], Train Loss: 0.0029, Val Loss: 0.0047\n","Train accuracy: 0.9440, Val accuracy: 0.9174\n","Epoch [11/50], Train Loss: 0.0028, Val Loss: 0.0047\n","Train accuracy: 0.9491, Val accuracy: 0.9183\n","Epoch [12/50], Train Loss: 0.0025, Val Loss: 0.0042\n","Train accuracy: 0.9532, Val accuracy: 0.9264\n","Epoch [13/50], Train Loss: 0.0023, Val Loss: 0.0045\n","Train accuracy: 0.9562, Val accuracy: 0.9191\n","Epoch [14/50], Train Loss: 0.0021, Val Loss: 0.0048\n","Train accuracy: 0.9591, Val accuracy: 0.9205\n","Epoch [15/50], Train Loss: 0.0020, Val Loss: 0.0048\n","Train accuracy: 0.9624, Val accuracy: 0.9221\n","Epoch [16/50], Train Loss: 0.0018, Val Loss: 0.0047\n","Train accuracy: 0.9648, Val accuracy: 0.9267\n","Epoch [17/50], Train Loss: 0.0017, Val Loss: 0.0048\n","Train accuracy: 0.9677, Val accuracy: 0.9227\n","Epoch [18/50], Train Loss: 0.0015, Val Loss: 0.0056\n","Train accuracy: 0.9704, Val accuracy: 0.9197\n","Epoch [19/50], Train Loss: 0.0014, Val Loss: 0.0056\n","Train accuracy: 0.9725, Val accuracy: 0.9184\n","Epoch [20/50], Train Loss: 0.0014, Val Loss: 0.0052\n","Train accuracy: 0.9744, Val accuracy: 0.9254\n","Epoch [21/50], Train Loss: 0.0012, Val Loss: 0.0052\n","Train accuracy: 0.9774, Val accuracy: 0.9204\n","Epoch [22/50], Train Loss: 0.0012, Val Loss: 0.0056\n","Train accuracy: 0.9773, Val accuracy: 0.9242\n","Epoch [23/50], Train Loss: 0.0010, Val Loss: 0.0060\n","Train accuracy: 0.9807, Val accuracy: 0.9228\n","Epoch [24/50], Train Loss: 0.0010, Val Loss: 0.0061\n","Train accuracy: 0.9819, Val accuracy: 0.9222\n","Epoch [25/50], Train Loss: 0.0009, Val Loss: 0.0074\n","Train accuracy: 0.9831, Val accuracy: 0.9042\n","Epoch [26/50], Train Loss: 0.0009, Val Loss: 0.0060\n","Train accuracy: 0.9841, Val accuracy: 0.9238\n","Epoch [27/50], Train Loss: 0.0008, Val Loss: 0.0061\n","Train accuracy: 0.9849, Val accuracy: 0.9241\n","Epoch [28/50], Train Loss: 0.0007, Val Loss: 0.0065\n","Train accuracy: 0.9871, Val accuracy: 0.9230\n","Epoch [29/50], Train Loss: 0.0007, Val Loss: 0.0069\n","Train accuracy: 0.9867, Val accuracy: 0.9225\n","Epoch [30/50], Train Loss: 0.0006, Val Loss: 0.0068\n","Train accuracy: 0.9884, Val accuracy: 0.9255\n","Epoch [31/50], Train Loss: 0.0006, Val Loss: 0.0076\n","Train accuracy: 0.9875, Val accuracy: 0.9196\n","Epoch [32/50], Train Loss: 0.0005, Val Loss: 0.0071\n","Train accuracy: 0.9899, Val accuracy: 0.9247\n","Epoch [33/50], Train Loss: 0.0006, Val Loss: 0.0074\n","Train accuracy: 0.9889, Val accuracy: 0.9205\n","Epoch [34/50], Train Loss: 0.0006, Val Loss: 0.0072\n","Train accuracy: 0.9896, Val accuracy: 0.9270\n","Epoch [35/50], Train Loss: 0.0005, Val Loss: 0.0072\n","Train accuracy: 0.9915, Val accuracy: 0.9248\n","Epoch [36/50], Train Loss: 0.0004, Val Loss: 0.0074\n","Train accuracy: 0.9918, Val accuracy: 0.9246\n","Epoch [37/50], Train Loss: 0.0004, Val Loss: 0.0077\n","Train accuracy: 0.9918, Val accuracy: 0.9231\n","Epoch [38/50], Train Loss: 0.0004, Val Loss: 0.0079\n","Train accuracy: 0.9935, Val accuracy: 0.9244\n","Epoch [39/50], Train Loss: 0.0004, Val Loss: 0.0078\n","Train accuracy: 0.9929, Val accuracy: 0.9245\n","Epoch [40/50], Train Loss: 0.0004, Val Loss: 0.0079\n","Train accuracy: 0.9938, Val accuracy: 0.9247\n","Epoch [41/50], Train Loss: 0.0003, Val Loss: 0.0078\n","Train accuracy: 0.9942, Val accuracy: 0.9269\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-39-5d0b9a79476f>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m train_losses, val_losses, train_accs, val_accs, best_model, best_ind, best_train_acc, best_val_acc, best_test_acc = train(\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseed_Conv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m )\n","\u001b[0;32m<ipython-input-38-02f8a0217a81>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loss_f, optimizer, n_epoch, train_dataloader, val_dataloader, test_dataloader, seed_value)\u001b[0m\n\u001b[1;32m     34\u001b[0m           \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Zero the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m           \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}